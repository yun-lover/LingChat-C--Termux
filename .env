# ==================================================
#                      配置文件
# ==================================================

[Server]
BACKEND_PORT="8765" 
DOCUMENT_ROOT="frontend"

[API_LLM]
# 在这里填入你自己模型提供商的 API Key
DEEPSEEK_API_KEY = "your api key"
# LLM API的基础URL
API_BASE_URL = "https://api.deepseek.com/v1"


# --- Embedding模型的配置  ---
[API_EMBEDDING]
# 在这里填入您的Embedding的API Key
EMBEDDING_API_KEY = "your api key"
# 在这里填入其API的基础URL
EMBEDDING_API_URL = "https://api.siliconflow.com/v1/"
# Embedding模型的具体名称
EMBEDDING_MODEL = "Qwen/Qwen3-Embedding-0.6B"
# 该模型的向量维度
EMBEDDING_VECTOR_DIMENSION = "1024"

[Database]
# 轻量级RAG的记忆存储文件，它将自动被创建
MEMORY_DB_PATH = "memory.json"

[AI]
MODEL="deepseek-chat" # 这里填写你所调用的模型名称
TEMPERATURE=0.7 # 这里填写的数值表示模型思维的发散程度，越低发散程度越高，反之亦然。
MAX_HISTORY_TURNS="10" # 这里则数值则表示模型的记忆长度，由于目前市面上绝大多数大模型api都是无状态的，所以我们每次调用模型都需要将上文一同告诉模型，但这样太费token,所以要加以限制，所以模型只会记得包括你这句话的前十句话，但不包括RAG系统。
ENABLE_RAG = false # 这里控制RAG的开关,目前RAG系统为实验性功能，可能无法使用


[Voice]
# 在这里填入您的语音合成API的URL(已废弃)
VOICE_API_URL=""

[Character]
CHARACTER_NAME="钦灵" # 这里是ai的名称，具体如何体现请看web界面
CHARACTER_IDENTITY="可爱的狼娘" # 这里是ai的身份，具体如何体现请看web界面

[UI]
BACKGROUND_DAY_PATH="assets/bg/白天.jpeg" # 这里是白天的背景，当然你也可以换成别的
BACKGROUND_NIGHT_PATH="assets/bg/夜晚.jpg" # 这里是夜晚的背景，同样你也可以换成别的
CHARACTER_SPRITE_DIR="assets/char/ling/" # 这里是人物立绘的文件夹，具体作用可见下文的立绘情绪标签映射表
SFX_DIR="assets/sfx/" #这里是音效文件夹，具体作用可见下文的音效情绪标签映射表
# 注意，所有包含前端资源的文件夹均在frontend中

[EmotionMap] # 这里是立绘情绪标签映射表，你可以将assets/char/ling/中的文件名针对该映射表进行修改(不包括后缀)
高兴=兴奋 # 这里实际在文件夹中的文件名应当是"兴奋.png",下面一样，英文的是我懒惰改了
害羞=害羞
生气=厌恶
无语=speechless
惊讶=surprised
哭泣=sad
担心=sad
情动=love
调皮=playful
认真=neutral
疑惑=neutral
尴尬=shy
紧张=sad
自信=happy
害怕=sad
厌恶=angry
羞耻=shy
旁白=兴奋 # 旁边讲话时的立绘，我不想做多人物了
default=兴奋 # 当情绪标签无法识别时默认返回的立绘

[EmotionSfxMap] # 这里是音效情绪标签映射表，工作逻辑同上，文件夹是assets/sfx/
高兴=喜爱.wav
害羞=喜爱.wav
生气=喜爱.wav

[SystemPrompt] # 这里是ai的系统提示词，你可以在项目的根目录下的prompt.txt中修改
PROMPT_FILE="prompt.txt"
    